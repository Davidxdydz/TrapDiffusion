from tqdm.auto import tqdm
import yaml
import pathlib
import os

os.environ["KERAS_BACKEND"] = "torch"
import keras
import random
import numpy as np
import time
from training.datasets import load_dataset
from dataclasses import dataclass, asdict
from typing import List
from models.pinn import build_model
from models.cpu import CPUSequential
from typing import Callable
from models.pinn import PhysicsLoss


class ParameterRange:
    def __init__(self, choices, discrete=None):
        """
        choices: eiter a list of possible values or a tuple of (lower, upper) bounds
        k: number of values to sample
        discrete: choices are choices or bounds
        dtype: int, float or str
        """
        self.dtype = None
        if all(map(lambda x: isinstance(x, int), choices)):
            self.dtype = int
        elif any(map(lambda x: isinstance(x, float), choices)):
            self.dtype = float
        elif all(map(lambda x: isinstance(x, str), choices)):
            self.dtype = str
        if discrete is None:
            if len(choices) != 2 or self.dtype == str:
                discrete = True
            else:
                discrete = False
        self.choices = choices
        self.discrete = discrete
        if not discrete:
            if len(choices) != 2:
                raise ValueError(
                    "Continuous parameters must have exactly two choices: lower and upper bound"
                )
            if self.dtype not in [int, float]:
                raise ValueError("Continuous parameters must have dtype int or float")

    def info(self):
        return dict(
            choices=self.choices,
            discrete=self.discrete,
            dtype=str(self.dtype),
        )

    def random(self, k=None):
        if self.discrete:
            if k == None:
                return random.choice(self.choices)
            else:
                return random.choices(self.choices, k=k)
        else:
            if k == None:
                if self.dtype == float:
                    return random.uniform(*self.choices)
                elif self.dtype == int:
                    return random.randint(*self.choices)
            else:
                result = []
                for _ in range(k):
                    if self.dtype == float:
                        result.append(random.uniform(*self.choices))
                    elif self.dtype == int:
                        result.append(random.randint(*self.choices))
                return result


@dataclass
class SearchModel:
    input_channels: int
    output_channels: int
    layer_sizes: List[int]
    activations: List[str]
    output_activation: str
    physics_weight: float
    epochs: int
    learning_rate: float
    batch_size: int
    ReduceLROnPlateau_patience: int
    ReduceLROnPlateau_factor: float
    EarlyStopping_patience: int
    optimizer: keras.Optimizer = None
    loss_function: Callable = None
    normalizer = False
    layer_count: int = None
    gpu_inference: float = None
    cpu_inference: float = None
    mass_loss_mean: float = None
    mass_loss_std: float = None
    max_mae: float = None
    mean_mae: float = None
    total_mae: float = None
    output_path: str = None
    param_count: int = None
    model: keras.Sequential = None
    name: str = "model"

    def build_model(self):
        self.model = build_model(
            intput_channels=self.input_channels,
            output_channels=self.output_channels,
            layer_sizes=self.layer_sizes,
            activations=self.activations,
            output_activation=self.output_activation,
            name=self.name,  # TODO find descriptive name for autogenerated models
            optimizer=self.optimizer,
            loss_function=self.loss_function,
            normalizer=self.normalizer,
        )
        self.param_count = self.model.count_params()
        self.cpu_bench()
        self.gpu_bench()

    def __post_init__(self):
        if self.model is None:
            self.build_model()
        if self.loss_function is None:
            self.loss_function = self.model.loss
        if self.optimizer is None:
            self.optimizer = self.model.optimizer
        if self.layer_count is None:
            self.layer_count = len(self.layer_sizes)

    @staticmethod
    def load(path):
        path = pathlib.Path(path)
        model = keras.models.load_model(
            path / "model.keras", custom_objects={"PhysicsLoss": PhysicsLoss}
        )
        with open(path / "info.yaml") as f:
            info = yaml.unsafe_load(f)
        return SearchModel(
            **info,
            model=model,
        )

    @staticmethod
    def dict_factory(data):
        """
        Remove not serializable data from dict
        """
        left_out = {"model", "optimizer", "loss_function"}
        return {k: v for k, v in data if k not in left_out}

    def info(self):
        return asdict(
            self,
            dict_factory=SearchModel.dict_factory,
        )

    def gpu_bench(self, batch=1000, average=10):
        """
        Inference time in seconds per sample
        """
        x = np.random.uniform(size=(batch, self.model.input_shape[1]))
        start = time.perf_counter()
        for _ in range(average):
            self.model.predict(x, batch_size=batch, verbose=0)
        end = time.perf_counter()
        self.gpu_inference = (end - start) / average / batch

    def cpu_bench(self, batch=1000, average=10):
        """
        Inference time in seconds per sample
        """
        x = np.random.uniform(size=(batch, self.model.input_shape[1]))
        cpu_model = CPUSequential(self.model)
        start = time.perf_counter()
        for _ in range(average):
            cpu_model.predict(x)
        end = time.perf_counter()
        self.cpu_inference = (end - start) / average / batch

    def train(self, dataset_name, dataset_dir="datasets", output_dir="", quiet=False):
        info, (x_train, y_train), (x_val, y_val) = load_dataset(
            dataset_name, dataset_dir
        )
        scheduler = keras.callbacks.ReduceLROnPlateau(
            patience=self.ReduceLROnPlateau_patience,
            factor=self.ReduceLROnPlateau_factor,
        )
        early_stopping = keras.callbacks.EarlyStopping(
            patience=self.EarlyStopping_patience,
            restore_best_weights=True,
        )
        self.model.fit(
            x_train,
            y_train,
            batch_size=self.batch_size,
            epochs=self.epochs,
            shuffle=True,
            validation_split=0.15,
            callbacks=[scheduler, early_stopping],
            verbose=0 if quiet else 1,
        )

    def evaluate(
        self, dataset_name, dataset_dir="datasets", output_dir="", quiet=False
    ):
        info, (x_train, y_train), (x_val, y_val) = load_dataset(
            dataset_name, dataset_dir
        )
        predictions = self.model.predict(x_val, batch_size=2**12, verbose=0)
        maes = np.mean(np.abs(predictions - y_val), axis=1)
        self.max_mae = float(np.max(maes))
        self.mean_mae = float(np.mean(maes))
        self.total_mae = float(np.sum(maes))
        if not info.get("pre_normalized", False):
            output_dim = y_val.shape[-1]
            correction_factors = y_train[:, -output_dim:]
            mass_loss = 1 - np.sum(predictions * correction_factors, axis=1)
        else:
            mass_loss = 1 - np.sum(predictions, axis=1)
        self.mass_loss_mean = np.mean(mass_loss)
        self.mass_loss_std = np.std(mass_loss)

    def save(self, output_path=None):
        if output_path is None:
            output_path = self.output_path
        self.output_path = pathlib.Path(output_path)
        self.output_path.mkdir(exist_ok=True, parents=True)
        self.model.save(self.output_path / "model.keras")
        with open(self.output_path / "info.yaml", "w") as f:
            yaml.dump(self.info(), f)

    def train_and_evaluate(
        self, dataset_name, dataset_dir="datasets", output_dir="", quiet=False
    ):
        self.train(dataset_name, dataset_dir, output_dir, quiet)
        self.evaluate(dataset_name, dataset_dir, output_dir, quiet)
        self.save(pathlib.Path(output_dir) / time.strftime("%Y-%m-%d-%H-%M-%S"))


class SearchModelGenerator:
    def __init__(
        self,
        input_channels: int,
        output_channels: int,
        layer_count: ParameterRange,
        layer_sizes: ParameterRange,
        activations: ParameterRange,
        output_activation: ParameterRange,
        physics_weight: ParameterRange,
        epochs=ParameterRange([20]),
        learning_rate=ParameterRange([3e-4]),
        batch_size=ParameterRange([2**12]),
        ReduceLROnPlateau_patience=ParameterRange([5]),
        ReduceLROnPlateau_factor=ParameterRange([0.5]),
        EarlyStopping_patience=ParameterRange([6]),
        pre_normalized=False,
        reject=lambda search_model: False,
        patience=100,
    ):
        self.input_dim = input_channels
        self.output_dim = output_channels
        self.epochs_range = epochs
        self.batch_size_range = batch_size
        self.learning_rate_range = learning_rate
        self.layer_sizes_range = layer_sizes
        self.layer_count_range = layer_count
        self.activations_range = activations
        self.output_activation_range = output_activation
        self.physics_weight_range = physics_weight
        self.ReduceLROnPlateau_patience_range = ReduceLROnPlateau_patience
        self.ReduceLROnPlateau_factor_range = ReduceLROnPlateau_factor
        self.EarlyStopping_patience_range = EarlyStopping_patience
        self.reject = reject
        self.pre_normalized = pre_normalized
        self.patience = patience

    def info(self):
        return dict(
            input_dim=self.input_dim,
            output_dim=self.output_dim,
            epochs_range=self.epochs_range.info(),
            batch_size_range=self.batch_size_range.info(),
            learning_rate_range=self.learning_rate_range.info(),
            layer_sizes_range=self.layer_sizes_range.info(),
            layer_count_range=self.layer_count_range.info(),
            activations_range=self.activations_range.info(),
            output_activation_range=self.output_activation_range.info(),
            physics_weight_range=self.physics_weight_range.info(),
            ReduceLROnPlateau_patience_range=self.ReduceLROnPlateau_patience_range.info(),
            ReduceLROnPlateau_factor_range=self.ReduceLROnPlateau_factor_range.info(),
            EarlyStopping_patience_range=self.EarlyStopping_patience_range.info(),
            pre_normalized=self.pre_normalized,
        )

    def random_model(self):
        for _ in range(self.patience):
            layer_count = self.layer_count_range.random()
            learning_rate = self.learning_rate_range.random()
            physics_weight = self.physics_weight_range.random()
            if self.pre_normalized:
                loss_function = keras.losses.MeanAbsoluteError()
                physics_weight = 0
            else:
                loss_function = PhysicsLoss(physics_weight)
            model = SearchModel(
                input_channels=self.input_dim,
                output_channels=self.output_dim,
                epochs=self.epochs_range.random(),
                batch_size=self.batch_size_range.random(),
                learning_rate=learning_rate,
                layer_sizes=self.layer_sizes_range.random(layer_count),
                activations=self.activations_range.random(layer_count),
                output_activation=self.output_activation_range.random(),
                physics_weight=physics_weight,
                ReduceLROnPlateau_patience=self.ReduceLROnPlateau_patience_range.random(),
                ReduceLROnPlateau_factor=self.ReduceLROnPlateau_factor_range.random(),
                EarlyStopping_patience=self.EarlyStopping_patience_range.random(),
                optimizer=keras.optimizers.Adam(learning_rate=learning_rate),
                loss_function=loss_function,
            )

            if not self.reject(model):
                return model
        raise RuntimeError(f"No suitable model config found in {self.patience} tries.")


def random_search(
    output_dir,
    dataset_name,
    generator: SearchModelGenerator,
    n: int,
    dataset_dir="datasets",
    quiet=False,
):
    output_dir = pathlib.Path(output_dir)
    output_dir.mkdir(exist_ok=True, parents=True)
    with open(output_dir / "info.yaml", "w") as f:
        yaml.dump(generator.info(), f)
    finished = len(os.listdir(output_dir)) - 1
    n -= finished
    for _ in tqdm(range(n), disable=quiet):
        model = generator.random_model()
        print(f"Training {model.info()}")
        model.train_and_evaluate(
            dataset_name=dataset_name,
            dataset_dir=dataset_dir,
            output_dir=output_dir,
            quiet=quiet,
        )
